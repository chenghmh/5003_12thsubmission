{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor,AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error,log_loss\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>flip_y</th>\n",
       "      <th>id</th>\n",
       "      <th>l1_ratio</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>n_classes</th>\n",
       "      <th>n_clusters_per_class</th>\n",
       "      <th>n_features</th>\n",
       "      <th>n_informative</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>penalty</th>\n",
       "      <th>random_state</th>\n",
       "      <th>scale</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>0</td>\n",
       "      <td>0.304083</td>\n",
       "      <td>417</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>327</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>1089</td>\n",
       "      <td>none</td>\n",
       "      <td>475</td>\n",
       "      <td>24.242009</td>\n",
       "      <td>0.409987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.077781</td>\n",
       "      <td>1</td>\n",
       "      <td>0.727744</td>\n",
       "      <td>578</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>373</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>790</td>\n",
       "      <td>l1</td>\n",
       "      <td>569</td>\n",
       "      <td>54.626302</td>\n",
       "      <td>3.950953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>2</td>\n",
       "      <td>0.745885</td>\n",
       "      <td>588</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1198</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>428</td>\n",
       "      <td>none</td>\n",
       "      <td>529</td>\n",
       "      <td>17.999964</td>\n",
       "      <td>0.368702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.057261</td>\n",
       "      <td>3</td>\n",
       "      <td>0.474605</td>\n",
       "      <td>829</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>313</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>877</td>\n",
       "      <td>none</td>\n",
       "      <td>103</td>\n",
       "      <td>82.257222</td>\n",
       "      <td>1.004559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.073728</td>\n",
       "      <td>4</td>\n",
       "      <td>0.395049</td>\n",
       "      <td>167</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>644</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>216</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>418</td>\n",
       "      <td>95.515601</td>\n",
       "      <td>0.802800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha    flip_y  id  l1_ratio  max_iter  n_classes  n_clusters_per_class  \\\n",
       "0  0.0001  0.074798   0  0.304083       417          4                     3   \n",
       "1  0.0010  0.077781   1  0.727744       578          4                     5   \n",
       "2  0.0100  0.030196   2  0.745885       588          2                     5   \n",
       "3  0.0010  0.057261   3  0.474605       829          6                     5   \n",
       "4  0.0010  0.073728   4  0.395049       167          8                     5   \n",
       "\n",
       "   n_features  n_informative  n_jobs  n_samples     penalty  random_state  \\\n",
       "0         327              7      -1       1089        none           475   \n",
       "1         373              7       1        790          l1           569   \n",
       "2        1198              6       2        428        none           529   \n",
       "3         313              7       4        877        none           103   \n",
       "4         644             11       2        216  elasticnet           418   \n",
       "\n",
       "       scale      time  \n",
       "0  24.242009  0.409987  \n",
       "1  54.626302  3.950953  \n",
       "2  17.999964  0.368702  \n",
       "3  82.257222  1.004559  \n",
       "4  95.515601  0.802800  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_assignment = pd.read_csv('./data/test.csv')\n",
    "df = pd.concat([df_train,df_assignment])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create several attribute\n",
    "#drop some useless attributes\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_assignment = pd.read_csv('./data/test.csv')\n",
    "df = pd.concat([df_train,df_assignment])\n",
    "df = df.drop('random_state',axis = 1)\n",
    "df = df.drop('id',axis=1)\n",
    "#df['n_jobs'] = np.where((df['n_jobs']==-1)&(df['n_classes']==2),1,df['n_jobs'])\n",
    "df['n_jobs'] = np.where(df['n_jobs']==-1,16,df['n_jobs'])\n",
    "df['att1'] = df['n_samples']*df['n_features']*df['n_classes']\n",
    "df['att2'] = df['n_clusters_per_class']*df['n_classes']/df['n_jobs']\n",
    "#df['att3'] = df['n_samples']*df['n_features']*df['max_iter']\n",
    "#df['att4'] = (df['n_features']/df['scale'])\n",
    "df['att5'] = df['n_samples']*df['n_features']*df['n_classes']*df['max_iter']/df['n_jobs']\n",
    "df = df.drop('alpha',axis=1)\n",
    "df = df.drop('n_clusters_per_class',axis=1)\n",
    "df = df.drop('l1_ratio',axis=1)\n",
    "df = df.drop('n_classes',axis=1)\n",
    "df = df.drop('max_iter',axis=1)\n",
    "df = df.drop('flip_y',axis=1)\n",
    "df = df.drop('n_features',axis=1)\n",
    "df = df.drop('n_informative',axis=1)\n",
    "#df = df.drop('n_jobs',axis=1)\n",
    "#df = df.drop('n_samples',axis=1)\n",
    "#df = df.drop('penalty',axis=1)\n",
    "df = df.drop('scale',axis=1)\n",
    "df_dummies = pd.get_dummies(df)\n",
    "df_train_dummies = df_dummies.iloc[:df_train.shape[0],:]\n",
    "y = df_train_dummies['time']\n",
    "X = df_train_dummies.drop('time',axis=1)\n",
    "df_assignment_dummies = df_dummies.iloc[df_train.shape[0]:,:]\n",
    "X_assignment = df_assignment_dummies.drop('time',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state =5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard scaler to scale the input\n",
    "scaler = StandardScaler()\n",
    "df_train_dummies_scale = scaler.fit_transform(X_train)\n",
    "df_test_dummies_scale = scaler.transform(X_test)\n",
    "df_assignment_dummies_scale = scaler.transform(X_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=0,\n",
       "             subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [250, 500, 1000], 'max_depth': [1, 2], 'learning_rate': [0.01, 0.5, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use gridsearchCV to seach hyperparameters\n",
    "#use GradientBoostingRegressor for prediction\n",
    "#y (which is the time) is log \n",
    "#the log result will become both negative and positive\n",
    "#major errors occure when y is large, so sample_weight is used to put more weight in large y\n",
    "\n",
    "parameters = { 'n_estimators':[250,500,1000],'learning_rate':[0.01,0.5,0.1],'max_depth':[1,2]}\n",
    "#parameters = { 'n_estimators':[250,500,1000],'learning_rate':[0.01,0.1]}\n",
    "\n",
    "#est = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(df_train_dummies_scale, np.log(y_train))\n",
    "est = GradientBoostingRegressor( random_state=0, loss='ls')\n",
    "#est = AdaBoostRegressor(DecisionTreeRegressor(max_depth=2),n_estimators=300, random_state=0)\n",
    "clf = GridSearchCV(est, parameters, cv=3)\n",
    "clf.fit(df_train_dummies_scale, np.log(y_train),sample_weight=y_train**3)\n",
    "#est = AdaBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=0, loss='linear')\n",
    "#print(cross_val_score(est,df_train_dummies_scale, np.log(y_train),cv=3))\n",
    "#est.fit(df_train_dummies_scale, np.log(y_train),sample_weight = y_train**2)\n",
    "#print(mean_squared_error((y_test), np.exp(est.predict(df_test_dummies_scale))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 1000}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the hyperparameters associated with the best estimator from gridsearchCV\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09014734 -0.05629051 -0.07539125]\n",
      "0.15822234011556038\n",
      "0.5716804355655117\n"
     ]
    }
   ],
   "source": [
    "#train the train set and test with the test set with those hyperparamters\n",
    "\n",
    "best_est = GradientBoostingRegressor(n_estimators = 1000,learning_rate = 0.5,max_depth=1, random_state=0, loss='ls')\n",
    "#best_est =AdaBoostRegressor(LinearRegression(),n_estimators=500, random_state=0,learning_rate=0.01)\n",
    "print(cross_val_score(best_est,df_train_dummies_scale, np.log(y_train),cv=3,scoring = 'neg_mean_squared_error'))\n",
    "best_est.fit(df_train_dummies_scale, np.log(y_train),sample_weight = y_train**3)\n",
    "print(mean_squared_error((y_train), np.exp(best_est.predict(df_train_dummies_scale))))\n",
    "print(mean_squared_error((y_test), np.exp(best_est.predict(df_test_dummies_scale))))\n",
    "\n",
    "#[-0.09014734 -0.05629051 -0.07539125]\n",
    "#0.15822234011556038\n",
    "#0.5716804355655117\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train again with whole set of data \n",
    "\n",
    "best_est = GradientBoostingRegressor(n_estimators = 1000,learning_rate = 0.5,max_depth=1, loss='ls')\n",
    "best_est.fit(scaler.transform(X), np.log(y),sample_weight = y**3)\n",
    "df_submission = pd.read_csv('./data/samplesubmission.csv')\n",
    "df_submission['time'] = np.exp(best_est.predict(df_assignment_dummies_scale))\n",
    "df_submission.to_csv('12thsubmission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
